{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applied Deep Learning Tutorial \n",
    "# Transfer Learning for Object Classification \n",
    "\n",
    "## Introduction\n",
    "In this tutorial, you will attempt to benefit from a model that has been pretrained for the same task but on a different dataset. You will deploy the first layers and their feature extraction capabilities of a converged network. This process is known as transfer learning.\n",
    "\n",
    "<img src=\"graphics/Katze.jpg\" width=\"700\"><br>\n",
    "<center> Fig. 1: Cat and dog in an image </center>\n",
    "\n",
    "## Core idea\n",
    "A pre-trained model is a saved network that was previously trained on a large dataset, typically on a large-scale image-classification task, such as [ImageNet](http://image-net.org/challenges/LSVRC/), and [COCO](http://cocodataset.org/#home). We can either use the pretrained model as it is for inference on the task it has been trained on or we can do transfer learning using the pretrained convents for further training on a new dataset with a possibly new output space. \n",
    "\n",
    "The intuition behind transfer learning is that if this model trained on a large and general enough dataset, this model will effectively serve as a generic model of the visual world and the semantic features present in the visual world and shared between all visual tasks. We can leverage these learned feature maps without having to train a large model on a large dataset by using these models as the basis of our own model specific to our task. There are 2 scenarios of transfer learning using a pretrained model:\n",
    "\n",
    "- Fine Tuning or Retraining: Unfreezing a few of the top layers of a frozen model base used for feature extraction, and jointly training both the newly added classifier layers as well as the last layers of the frozen model. This allows us to \"fine tune\" the higher order feature representations in addition to our final classifier in order to make them more relevant for the specific task involved.\n",
    "- Feature Extraction: Use the representations learned by a previous model to extract meaningful features from new samples. We simply add a new output layer, which will be trained from scratch, on top of the pretrained model so that we can repurpose the feature maps learned previously for our dataset and our new output space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "Import the necessary libraries and load the [Dogs vs Cats](https://www.kaggle.com/c/dogs-vs-cats) dataset from Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "#print(\"TensorFlow version is \", tf.__version__)\n",
    "\n",
    "import numpy as np\n",
    "#import cv2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# Load Cats vs Dogs dataset\n",
    "zip_file = tf.keras.utils.get_file(origin=\"https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\",\n",
    "                                   fname=\"cats_and_dogs_filtered.zip\", extract=True)\n",
    "\n",
    "base_dir, _ = os.path.splitext(zip_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data\n",
    "Create directories for training and validation for both classes, such as dog and cat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training cat images: 1000\n",
      "Total training dog images: 1000\n",
      "Total validation cat images: 500\n",
      "Total validation dog images: 500\n"
     ]
    }
   ],
   "source": [
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "\n",
    "# Directory with our training cat pictures\n",
    "train_cats_dir = os.path.join(train_dir, 'cats')\n",
    "print ('Total training cat images:', len(os.listdir(train_cats_dir)))\n",
    "\n",
    "# Directory with our training dog pictures\n",
    "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
    "print ('Total training dog images:', len(os.listdir(train_dogs_dir)))\n",
    "\n",
    "# Directory with our validation cat pictures\n",
    "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
    "print ('Total validation cat images:', len(os.listdir(validation_cats_dir)))\n",
    "\n",
    "# Directory with our validation dog pictures\n",
    "validation_dogs_dir = os.path.join(validation_dir, 'dogs')\n",
    "print ('Total validation dog images:', len(os.listdir(validation_dogs_dir)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will set up a pipeline for data augmentation with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "image_size = 200 # All images will be resized to 160x160\n",
    "batch_size = 32\n",
    "\n",
    "# Rescale all images by 1./255 and apply image augmentation\n",
    "train_datagen = keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "validation_datagen = keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Flow training images in batches of 20 using train_datagen generator\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "                train_dir,  # Source directory for the training images\n",
    "                target_size=(image_size, image_size),\n",
    "                batch_size=batch_size,\n",
    "                # Since we use binary_crossentropy loss, we need binary labels\n",
    "                class_mode='binary')\n",
    "\n",
    "# Flow validation images in batches of 20 using test_datagen generator\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "                validation_dir, # Source directory for the validation images\n",
    "                target_size=(image_size, image_size),\n",
    "                batch_size=batch_size,\n",
    "                class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing pretrained model\n",
    "We will create the base model from the [VGG16](https://arxiv.org/pdf/1409.1556.pdf) model, the first model to introduce skip-connections, and pre-trained on the [ImageNet](http://image-net.org/challenges/LSVRC/) dataset, a large dataset of 1.4M images and 1000 classes of web images. This is a powerful model. Let's see what the features that it has learned can do for our cat vs. dog problem.\n",
    "\n",
    "You can find more pretrained and ready to load models [here](https://www.tensorflow.org/api_docs/python/tf/keras/applications).\n",
    "\n",
    "First, we need to pick which intermediate layer of the model we will use for feature extraction. A common practice is to use the output of the very last layer before the flatten operation, the so-called \"bottleneck layer\". The reasoning here is that the following fully-connected layers will be too specialized to the task the network was trained on, and thus the features learned by these layers won't be very useful for a new task. The bottleneck features, however, retain much generality.\n",
    "\n",
    "\n",
    "Let's instantiate a VGG16 model pre-loaded with weights trained on ImageNet. By specifying the include_top=False argument, we load a network that doesn't include the classification layers, which is ideal for feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SHAPE = (image_size, image_size, 3)\n",
    "\n",
    "# Create the base model from the pre-trained model MobileNet V2\n",
    "feature_extractor = tf.keras.applications.VGG16(input_shape=IMG_SHAPE,\n",
    "                                                include_top=False,\n",
    "                                                weights='imagenet')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "\n",
    "We will freeze the layers of the VGG16 and utilize the feature extractor capabilities of this part of the network. By adding a classification layer on top of it and training the top-level classifier on our data we repurpose the pretrained model.\n",
    "Freezing means keeping the respective weights from updating in the weight update phase of the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 200, 200, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 200, 200, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 200, 200, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 100, 100, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 100, 100, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 100, 100, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 50, 50, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 50, 50, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 50, 50, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 50, 50, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 25, 25, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 25, 25, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 25, 25, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 25, 25, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 12, 12, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 12, 12, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 12, 12, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 12, 12, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 6, 6, 512)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 0\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "feature_extractor.trainable = False\n",
    "\n",
    "# Let's take a look at the base model architecture (notice the amount of non-trainable params)\n",
    "feature_extractor.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are adding a classification layer to the base model. Compile the newly combined model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vgg16 (Functional)          (None, 6, 6, 512)         14714688  \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 512)              0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,715,201\n",
      "Trainable params: 513\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "  feature_extractor,\n",
    "  keras.layers.GlobalAveragePooling2D(),\n",
    "  keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.01),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can already train our classification layer based on the base model.\n",
    "Notice how few epochs are necessary to reach a decent performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "62/62 [==============================] - 384s 6s/step - loss: 0.5313 - accuracy: 0.7536 - val_loss: 0.3964 - val_accuracy: 0.8538\n",
      "Epoch 2/2\n",
      "62/62 [==============================] - ETA: 0s - loss: 0.3752 - accuracy: 0.8506"
     ]
    }
   ],
   "source": [
    "# Saving the model\n",
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_training_vgg16\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)\n",
    "\n",
    "epochs = 2 #the pretrained model ckpt_training_vgg16 has been trained for 2 epochs reaching a validation accuracy of: 0.8841\n",
    "steps_per_epoch = train_generator.n // batch_size\n",
    "validation_steps = validation_generator.n // batch_size\n",
    "\n",
    "history = model.fit(train_generator,\n",
    "                    steps_per_epoch = steps_per_epoch,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=validation_generator,\n",
    "                    validation_steps=validation_steps,\n",
    "                    callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps to take it from here\n",
    "\n",
    "- Search a fun dataset for object classification and try fine-tuning and feature extraction. Which approach does work best, why? Which one would you prefer over the other and why?\n",
    "- Can you think of a reason why someone would train a model from scratch now that you know about Transfer Learning?\n",
    "- Try deploying another base model. Can you point out differences in the transfer learning process. What are the characteristics you should be aware of when selecting a base model?\n",
    "- How would you use a base model in a time-series problem? Try deploying a model in that way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
